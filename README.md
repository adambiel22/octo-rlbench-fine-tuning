# octo-rlbench-fine-tuning
Fine-tuning Octo model with data generated by RLBench


## Step 1. Generate and Prepare RLBench Data

If you already have the data in [RLDS](https://research.google/blog/rlds-an-ecosystem-to-generate-share-and-use-datasets-in-reinforcement-learning/) format you can skip this step.

Otherwise, use [rlbench_dataset_builder](https://github.com/mateuszwyszynski/rlbench_dataset_builder) to generate the data.
It is provided as a submodule to this repository.
You can run `git submodule update --init --recursive` to get the most recent version.
Follow the instructions there to generate the data in RLDS format.


## Step 2. Setup the Environment

In a virtual environment:

1. Install octo and necessary dependencies:

```bash
pip install -e octo
pip install -r octo/requirements.txt
pip install --upgrade "jax[cuda12_pip]==0.4.20" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
pip install tensorflow[and-cuda]
pip install tensorflow==2.15.0
pip install gymnasium
```

2. Install [RLBench](https://github.com/stepjam/RLBench?tab=readme-ov-file#install)

Notes:

 - You can add the following snippet at the end of the `/bin/activate` script to automatically export necessary variables everytime you activate the environment.

```bash
# set env variables
export COPPELIASIM_ROOT=${HOME}/CoppeliaSim
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$COPPELIASIM_ROOT
export QT_QPA_PLATFORM_PLUGIN_PATH=$COPPELIASIM_ROOT
```

## Training

Training is performed using `finetune_rl_bench.py` script.
Example usage:

```bash
python3 finetune_rl_bench.py \
  --pretrained_path=hf://rail-berkeley/octo-base-1.5 \
  --data_dir=</absolute/path/to/tensorflow_datasets> \
  --save_dir=</aboslut/path/where/the/checkpoints/will/be/saved> \
  --batch_size=2
```

## Evaluation

You can evaluate the model with the `evaluate_rl_bench.py` script.
Example usage:

```bash
python evaluate_rl_bench.py --finetuned_path=<path/to/the/checkpoint>
```
