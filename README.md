# octo-rlbench-fine-tuning
Fine-tuning Octo model with data generated by RLBench


## Step 1. Generate and Prepare RLBench Data

If you already have the data in [RLDS](https://research.google/blog/rlds-an-ecosystem-to-generate-share-and-use-datasets-in-reinforcement-learning/) format you can skip this step.

Otherwise, use [rlbench_dataset_builder](https://github.com/mateuszwyszynski/rlbench_dataset_builder) to generate the data.
It is provided as a submodule to this repository.
You can run `git submodule update --init --recursive` to get the most recent version.
Follow the instructions there to generate the data in RLDS format.


## Step 2. Train the model

1. Copy the dataset in RLDS format to the entropy cluster. Eg.:

```bash
scp -r ~/tensorflow_datasets/rl_bench_dataset entropy_username@entropy.mimuw.edu.pl:/home/entropy_username/tensorflow_datasets
```

2. Login to entropy. Eg.:

```bash
ssh entropy_username@entropy.mimuw.edu.pl
```

3. Clone or copy the repository.

```bash
git clone git@github.com:adambiel22/octo-rlbench-fine-tuning.git
cd octo-rlbench-fine-tuning
```

4. Create python virtual environment.

```bash
cd octo
python3 -m venv octo-venv
source octo-venv/bin/activate
pip install -e .
pip install -r requirements.txt
pip install --upgrade "jax[cuda12_pip]==0.4.20" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
pip install tensorflow[and-cuda]
pip install tensorflow==2.15.0
```

5. Log in to WandB to track the training.

6. Check the `sbatch' script `fine_tuning_job.sh`. For `sbatch` details see [https://entropy-doc.mimuw.edu.pl/submittingjobs.html#using-sbatch](https://entropy-doc.mimuw.edu.pl/submittingjobs.html#using-sbatch). Make sure you specify the correct `partition', `qos' and all paths.
```bash
#!/bin/bash
#
#SBATCH --job-name=fine_tune_octo
#SBATCH --partition=a6000
#SBATCH --qos=2gpu3d
#SBATCH --gres=gpu:1
#SBATCH --time=3:00:00
#SBATCH --output=fine_tune_octo.txt

# full fine-tuning rlbench
python examples/02_finetune_new_observation_action_rl_bench.py \
  --pretrained_path=hf://rail-berkeley/octo-base-1.5 \
  --data_dir=~/tensorflow_datasets \
  --save_dir=~/octo-rlbench-fine-tuning/octo/checkpoint_rlbench \
  --batch_size=60
```

7. Run sbatch script.
```bash
sbatch fine_tuning_job.sh
```

The training can take up to 90 minutes. For testing purposes you can reduce number of iterations at the bottom of `examples/02_finetune_new_observation_action_rl_bench.py` file.

## Step 3. Evaluate the model

1. Navigate to `octo-rlbench-fine-tuning/octo` directory in your local machine.
2. Create python virtual environment.
```bash
python3 -m venv octo-venv-eval
source octo-venv-eval/bin/activate
pip install -e .
pip install -r requirements.txt
pip install --upgrade "jax==0.4.20"
pip install --upgrade "jaxlib==0.4.20"
pip install gymnasium

export COPPELIASIM_ROOT=${HOME}/CoppeliaSim
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$COPPELIASIM_ROOT
export QT_QPA_PLATFORM_PLUGIN_PATH=$COPPELIASIM_ROOT

pip install -e ../RLBench/
```

3. Copy the checkpoint from the Entropy cluster. Eg.:
```bash
scp -r entropy_username@entropy.mimuw.edu.pl:/home/entropy_username/octo-rlbench-fine-tuning/octo/checkpoint_rlbench .
```

4. Log in to WandB to track the evaluation.

5. Run evaluation
```bash
python examples/03_eval_finetuned_rlbench.py --finetuned_path=checkpoint_rlbench
```
